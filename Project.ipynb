{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f4bd8e-ea18-4bf5-9cc3-098e17e724d7",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c216e02c-a599-4a9a-84eb-1142454792d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score # TODO \n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV #TODO\n",
    "from sklearn.linear_model import LinearRegression # TODO\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c2934-22a1-4219-bd9d-f33eec553450",
   "metadata": {},
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ab3f8e-214a-43c8-b08a-b26a52678935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random state\n",
    "random_state = min(332078,332464)\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5208fc-ef5a-44cf-b47b-360a6dfa60f3",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb55e7c8-d437-4f8c-a785-7cb1149544ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv('development.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2396a9e-5932-498f-a406-c190268daa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "df_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c341c-263e-4328-a64f-6836947595dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "df_dev.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb021ee-f3de-4db3-8ad3-877940742620",
   "metadata": {},
   "source": [
    "1) x, y: the position of the events over the sensor\n",
    "2) pmax[0], pmax[1], ... pmax[17]: the magnitude of the positive peak of the signal, in mV\n",
    "3) negpmax[0], negpmax[1], ... negpmax[17]: the magnitude of the negative peak of the signal, in mV\n",
    "4) tmax[0], tmax[1], ... tmax[17]: the delay (in ns) from a reference time when the positive peak of the signal\n",
    "5) area[0], area[1], ... area[17]: the area under the signal\n",
    "6) rms[0], rms[1], ... rms[17]: the root mean square (RMS) value of the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f3a28-6dc6-48fe-ad8d-ff1b7cd645bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN values\n",
    "display(df_dev.isna().any())\n",
    "print(f'\\nThe dataset has {df_dev.isna().any().sum()} NaN values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724bbed-d85e-4331-acef-0f2989fe90b9",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7355a2-be2f-48dd-bc13-b0eb6757bf94",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506b4eb-1a2e-48d2-ac7f-239a85c9c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of x and y\n",
    "df_dev.plot.scatter('x','y', s=20, alpha=0.5)\n",
    "plt.xlabel('$X$')\n",
    "plt.ylabel('$Y$',rotation=0)\n",
    "plt.title('Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a4df3-edbd-41cb-8414-6120838451a6",
   "metadata": {},
   "source": [
    "Some areas of the sensor are not covered by any event. That occurs because, at those coordinates, either pads or wires used to read the signals from the pads (due to their reflective properties) are presen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bda5aa-441e-4e83-9a58-65b14dd844d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot of the features\n",
    "for elem in ['pmax','negpmax','area','tmax','rms']:\n",
    "    rows, cols = 6, 3\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(25, 20))\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    i=0\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            ax[row, col].plot(df_dev[f'{elem}[{i}]'])\n",
    "            ax[row,col].set_title(f'{elem}[{i}]')\n",
    "            ax[row,col].set_xlabel('events')\n",
    "            ax[row,col].set_ylabel('magnitude')\n",
    "            i+=1\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fcd7e-01ac-442b-aee2-c059f14517a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sensor position with respect to the pmax feature\n",
    "elem = 'pmax'\n",
    "rows, cols = 6, 3\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(25, 20))\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "i=0\n",
    "for row in range(rows):\n",
    "    for col in range(cols):\n",
    "        limit = df_dev[f'{elem}[{i}]'].mean() + 2*df_dev[f'{elem}[{i}]'].std()\n",
    "        mask = df_dev[f'{elem}[{i}]'] > limit\n",
    "        ax[row, col].scatter(df_dev['x'],df_dev['y'], s=15, alpha=0.5)\n",
    "        ax[row, col].scatter(df_dev[mask]['x'],df_dev[mask]['y'],c='orange', s=15)\n",
    "        ax[row,col].set_title(f'Sensor [{i}]')\n",
    "        ax[row,col].set_xlabel('x')\n",
    "        ax[row,col].set_ylabel('y',rotation=0)\n",
    "        ax[row, col].set_xlim(200, 600)\n",
    "        ax[row, col].set_ylim(200, 600)\n",
    "        i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98ca97-f91f-46eb-af7c-2dd0c2e99b44",
   "metadata": {},
   "source": [
    "### Remove noise features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0793749-33aa-4a23-9c5a-b462e36b46ee",
   "metadata": {},
   "source": [
    "Thanks to the previous graph, we can deduce that the noise is caused by features: 0, 7, 12, 15, 16, and 17. As for feature 15, it was not easy to determine whether it was actually a source of noise. However, thanks to the last graph, we can see that the alleged pad 15 is able to detect high peaks on distant sensor areas. This implies that it does indeed constitute a source of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e15382e-219c-4a31-b3af-535c41069d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of feature indices with noise\n",
    "noise_features = [0,7,12,15,16,17]\n",
    "# list of feature indices without noise\n",
    "no_noise_features = [elem for elem in np.arange(18) if elem not in noise_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af48b86e-867c-46c8-87bd-6680e0fe435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise features\n",
    "for i in noise_features:\n",
    "   df_dev = df_dev.drop(columns=[f'pmax[{i}]',f'negpmax[{i}]',f'area[{i}]',f'tmax[{i}]',f'rms[{i}]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4505095-ff98-4257-8bcd-c6f3d8d4f32d",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebb9e7-c5f1-4037-b083-42656f5d7ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 15))\n",
    "\n",
    "# first row\n",
    "for idx, elem in enumerate(['pmax', 'negpmax']):\n",
    "    df_dev[[f'{elem}[{i}]' for i in no_noise_features]].boxplot(ax=axes[0, idx])\n",
    "    axes[0, idx].set_xticklabels(axes[0, idx].get_xticklabels(), rotation=75)\n",
    "    axes[0, idx].set_title(elem)\n",
    "\n",
    "# second row\n",
    "for idx, elem in enumerate(['area', 'tmax']):\n",
    "    df_dev[[f'{elem}[{i}]' for i in no_noise_features]].boxplot(ax=axes[1, idx])\n",
    "    axes[1, idx].set_xticklabels(axes[1, idx].get_xticklabels(), rotation=75)\n",
    "    axes[1, idx].set_title(elem)\n",
    "\n",
    "# third row\n",
    "for idx, elem in enumerate(['rms']):\n",
    "    df_dev[[f'{elem}[{i}]' for i in no_noise_features]].boxplot(ax=axes[2, idx])\n",
    "    axes[2, idx].set_xticklabels(axes[2, idx].get_xticklabels(), rotation=75)\n",
    "    axes[2, idx].set_title(elem)\n",
    "\n",
    "# remove empty subplot in the third row\n",
    "fig.delaxes(axes[2, -1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2669170c-0805-4e1f-a90f-586bb92ee582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines before removing positive values from negpmax: 385500\n",
      "Number of lines after removing positive values from negpmax: 385497\n"
     ]
    }
   ],
   "source": [
    "# remove positive negpmax values\n",
    "print('Number of lines before removing positive values from negpmax:', df_dev.shape[0])\n",
    "\n",
    "for i in no_noise_features:\n",
    "    df_dev = df_dev[df_dev[f'negpmax[{i}]'] < 0]\n",
    "print('Number of lines after removing positive values from negpmax:', df_dev.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527fb52-59be-4ec1-9eb3-57a98341fa59",
   "metadata": {},
   "source": [
    "##### ZSCORE OVER SEPARATE SENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f6ef34-8e8f-45c1-b2b2-95cb96a0e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensor = df_dev.copy()\n",
    "\n",
    "for i in no_noise_features:\n",
    "    list_values = []\n",
    "    for elem in ['pmax','negpmax','area','tmax','rms']:\n",
    "        values = df_sensor[[f'{elem}[{i}]']].values\n",
    "        list_values.append((values - values.mean()) / values.std())\n",
    "    df_sensor[f'zscore[{i}]'] = np.array(list_values).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a10bc0cf-769f-455e-bde9-d766a0f2eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zscores = [col for col in df_sensor.columns if col.startswith('zscor')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7441e070-8b0f-499b-b7c2-efde2498bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_index_zscore(df, threshold):\n",
    "    return df[(df > threshold) | (df < -threshold)].index\n",
    "\n",
    "threshold = 0.2\n",
    "index_dict = {f'index_s{i}zscore': get_index_zscore(df_sensor[f'zscore[{i}]'], threshold) for i in no_noise_features}\n",
    "\n",
    "index_list_zscore = np.concatenate([index for _, index in index_dict.items()])\n",
    "sensor_counter = Counter(index_list_zscore)\n",
    "index_to_remove = [key for key, count in sensor_counter.items() if count == 12]\n",
    "\n",
    "last_dim = df_no_outliers.shape[0]\n",
    "df_no_outliers = df_sensor.drop(index_to_remove).drop(columns=zscores)\n",
    "\n",
    "print(f'Non-outlier observations: {df_no_outliers.shape[0]}')\n",
    "print(f'Identified outliers: {last_dim - df_no_outliers.shape[0]}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa09f93-66c8-4e0b-98d3-c43453c043a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02b909-d5d0-43fa-8445-f4cde89a02b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### MAHALANOBIS DISTANCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38b6bf-e29a-436f-98ee-344401a20bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = pd.DataFrame(df_dev)\n",
    "\n",
    "# calculate mean and inverse covariance matrix\n",
    "mean = df_no_outliers.drop(columns=['x','y']).mean()\n",
    "\n",
    "covariance_matrix = df_no_outliers.drop(columns=['x','y']).cov()\n",
    "\n",
    "cov_inv = pd.DataFrame(np.linalg.inv(covariance_matrix.values), \n",
    "                       columns = df_no_outliers.drop(columns=['x','y']).columns, \n",
    "                       index = df_no_outliers.drop(columns=['x','y']).columns)\n",
    "# calculate Mahalanobis distance for each data point\n",
    "df_no_outliers['mahalanobis_distance'] = df_no_outliers.drop(columns=['x','y']).apply(lambda x: mahalanobis(x, mean, cov_inv), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038c51c-8b5b-4dff-892d-88d31f20534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = df_no_outliers['mahalanobis_distance'].mean() +  3.5 * df_no_outliers['mahalanobis_distance'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19362dfb-7c40-4a89-8ced-00457c72bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mahalanobis_distance over the events\n",
    "df_no_outliers['mahalanobis_distance'].plot(linewidth=0.25, label='Mahalanobis Distance')\n",
    "plt.axhline(y=threshold, color='red', linestyle='--', label='Threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02d85a-d919-4e74-9417-b63a99b488a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Mahalanobis Distance distribution\n",
    "plt.hist(df_no_outliers['mahalanobis_distance'], bins=150, color='blue', alpha=0.7)\n",
    "plt.axvline(x=threshold, color='red', linestyle='--', label='Threshold')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Mahalanobis Distance Distribution with Threshold')\n",
    "plt.xlabel('Mahalanobis Distance')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23da75-b07b-4052-a4cf-7cbf8070006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = df_no_outliers[df_no_outliers['mahalanobis_distance'] < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c430ad9-d6d7-4c12-a89f-71d5d0d3ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "385497 - df_no_outliers.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c8b00-fdf0-4129-994c-29080300e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = df_no_outliers.drop(columns='mahalanobis_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f36c38-2909-4972-977d-15d6576ee397",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### ZSCORE OUTLIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db7df5-0e2a-43ff-aea1-01924248d2d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_outliers = pd.DataFrame(df_dev)\n",
    "\n",
    "n_outliers = 0\n",
    "for elem in ['pmax','negpmax','area','tmax','rms']:\n",
    "    for i in no_noise_features:\n",
    "        values = df_no_outliers[[f'{elem}[{i}]']]\n",
    "        df_no_outliers['zscore'] = (values - values.mean()) / values.std()\n",
    "\n",
    "        l=15\n",
    "        \n",
    "        outliers = df_no_outliers[(df_no_outliers['zscore']< -l) | (df_no_outliers['zscore']> l)]\n",
    "        n_outliers += len(outliers)\n",
    "        \n",
    "        df_no_outliers = df_no_outliers[(df_no_outliers['zscore'] >= -l) & (df_no_outliers['zscore']<= l)]\n",
    "        \n",
    "df_no_outliers = df_no_outliers.drop(columns=['zscore'])\n",
    "print('Non-outlier observations: %d' % len(df_no_outliers))\n",
    "print('Identified outliers: %d' % n_outliers)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e962e9-1563-462e-a9d8-95a8d94a9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1a3a9-632e-4cd0-b435-edbe143a977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 15))\n",
    "\n",
    "# Boxplots for the first row\n",
    "for idx, elem in enumerate(['pmax', 'negpmax']):\n",
    "    df_dev[[f'{elem}[{i}]' for i in no_noise_features]].boxplot(ax=axes[0, idx])\n",
    "    axes[0, idx].set_xticklabels(axes[0, idx].get_xticklabels(), rotation=75)\n",
    "    axes[0, idx].set_title(elem)\n",
    "\n",
    "# Boxplots for the second row\n",
    "for idx, elem in enumerate(['area', 'tmax']):\n",
    "    df_dev[[f'{elem}[{i}]' for i in no_noise_features]].boxplot(ax=axes[1, idx])\n",
    "    axes[1, idx].set_xticklabels(axes[1, idx].get_xticklabels(), rotation=75)\n",
    "    axes[1, idx].set_title(elem)\n",
    "\n",
    "# Boxplots for the third row\n",
    "for idx, elem in enumerate(['rms']):\n",
    "    df_dev[[f'{elem}[{i}]' for i in no_noise_features]].boxplot(ax=axes[2, idx])\n",
    "    axes[2, idx].set_xticklabels(axes[2, idx].get_xticklabels(), rotation=75)\n",
    "    axes[2, idx].set_title(elem)\n",
    "\n",
    "# Remove empty subplot in the third row\n",
    "fig.delaxes(axes[2, -1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509a9b9-842f-4924-bf62-13577ad958ce",
   "metadata": {},
   "source": [
    "### Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490deb2-0d97-4bea-9173-2f042c0e6c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = df_no_outliers.drop(columns=['x', 'y']).values\n",
    "y = df_no_outliers[['x', 'y']].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size= 0.2, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd6c00-708e-4a51-ac9f-617585aa8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest x feature selection\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=random_state, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cde8f-49cd-466b-ac41-07c9e3220fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# fit the model\n",
    "reg.fit(X_train , y_train)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "print(f'Execution Time: {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f541e-30a7-43ff-8dc7-7684dbb69ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 score\n",
    "print('R^2 score:',r2_score(y_valid, reg.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c37d94-497f-4aed-bc7b-322af1958a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances\n",
    "feature_names = df_no_outliers.drop(columns=['x', 'y']).columns\n",
    "\n",
    "feature_importances_list = sorted(zip(feature_names, reg.feature_importances_), key = lambda x: x[1], reverse = True)\n",
    "feature_importances_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea243826-6510-421d-b66c-229c2c4b4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importances\n",
    "\n",
    "# extract feature names and importances from the list\n",
    "feature_names, importances = zip(*feature_importances_list)\n",
    "\n",
    "# Plotting the vertical bar chart\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(feature_names, importances)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.xticks(rotation=75, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd271a7-5088-43c5-af58-53d106eae6a3",
   "metadata": {},
   "source": [
    "As indicated by the importance given to the different features, it is evident that the worst categories are represented by tmax, rms, and area. Therefore, we will proceed with the removal of these features in order to reduce the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd8766a6-7614-4403-927f-f1ef54dd539b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# removing tmax, area and rms feature\n",
    "for i in no_noise_features:\n",
    "    df_no_outliers = df_no_outliers.drop(columns=[f'tmax[{i}]',f'rms[{i}]', f'area[{i}]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e76d4c-f784-4f07-abc8-aefe9cb4062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the dataset\n",
    "df_clean = df_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6933f3-b996-4292-8e21-b277a84fd16a",
   "metadata": {},
   "source": [
    "# Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09be447-40f2-4d39-9052-bca182859e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splitting\n",
    "X = df_clean.drop(columns=['x', 'y']).values\n",
    "y = df_clean[['x', 'y']].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size= 0.2, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8b77a-f8d0-44d3-8679-34acbb2bbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test scale and splitting \n",
    "\n",
    "# standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_scaled, X_valid_scaled, y_train_scaled, y_valid_scaled = train_test_split(X_scaled, y, test_size= 0.2, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadfaca-dfc6-4d5e-a9ca-bf3e4fc4508a",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b6926-57f2-433a-b627-c2b68086aab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the params\n",
    "param_grid = {\n",
    "    'n_estimators': [i for i in range(800,1100,100)],\n",
    "    'criterion': ['squared_error'],\n",
    "    'max_features': ['sqrt'],\n",
    "    'random_state': [random_state],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(RandomForestRegressor(), param_grid, scoring='r2', n_jobs=-1, cv=5)\n",
    "\n",
    "#start\n",
    "start_time = time.time()\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "#end\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Execution Time: {end_time - start_time} seconds')\n",
    "print(f'GridSearchCV best params: {gs.best_params_}')\n",
    "\n",
    "######## [i for i in range(100,500,100)] ################################\n",
    "# Execution Time: 1421.2425608634949 seconds \n",
    "# GridSearchCV best params: {'criterion': 'squared_error', \n",
    "#                            'max_features': 'sqrt', \n",
    "#                            'n_estimators': 400, \n",
    "#                            'random_state': 332078}\n",
    "#\n",
    "#                      ------- score: 4.918 -------\n",
    "#\n",
    "##########################################################################\n",
    "\n",
    "######## [i for i in range(500,800,100)] ################################\n",
    "#Execution Time: 2413.7543437480927 seconds\n",
    "# GridSearchCV best params: {'criterion': 'squared_error', \n",
    "#                            'max_features': 'sqrt', \n",
    "#                            'n_estimators': 700, \n",
    "#                            'random_state': 332078}\n",
    "#\n",
    "#                      ------- score: 4.903 -------\n",
    "#\n",
    "##########################################################################\n",
    "\n",
    "######## [i for i in range(800,1100,100)] ################################\n",
    "# GridSearchCV best params: {'criterion': 'squared_error', \n",
    "#                            'max_features': 'sqrt', \n",
    "#                            'n_estimators': 1000, \n",
    "#                            'random_state': 332078}\n",
    "#\n",
    "#                      ------- score: 4.899 -------\n",
    "#\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b73a4d-5c95-48ec-8bd0-891042b70062",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(n_estimators=2500, \n",
    "                            criterion='squared_error', \n",
    "                            max_features='sqrt', \n",
    "                            random_state=random_state)\n",
    "\n",
    "#start training\n",
    "start_time = time.time()\n",
    "\n",
    "reg.fit(X_train , y_train)\n",
    "\n",
    "#end training\n",
    "end_time = time.time()\n",
    "print(f'Execution Time: {end_time - start_time} seconds')\n",
    "\n",
    "# predict\n",
    "y_pred = reg.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc001a4-2740-4ef2-a7fa-22b95d6f16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "reg = gs.best_estimator_\n",
    "# predict\n",
    "y_pred = reg.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f421aa0-f211-4c2f-a962-3c42282f79f0",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8363453-61be-41c6-bd90-3c3bf739a0c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chose the best weights and algorithm\n",
    "param_grid = {'n_neighbors': [i for i in range(5,50,5)],\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'algorithm':['auto', 'ball_tree', 'kd_tree'],\n",
    "             }\n",
    "\n",
    "\n",
    "gs = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "#start training \n",
    "start_time = time.time()\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "#end training\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Execution Time: {end_time - start_time} seconds')\n",
    "print(f'GridSearchCV best params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d66cf-7920-4596-8008-ebc0c91d371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the best model\n",
    "kkn = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3189350-68f0-43f9-9a7f-304806e70ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the local prediction\n",
    "y_pred = kkn.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a239a-ee4f-40d0-8f08-3d6f00e591f2",
   "metadata": {},
   "source": [
    "After applying grid search on the model we saw that the best model parameters are: {}.\n",
    "\n",
    "With this model we obtained a score, considering the Euclidean distance, of 5.573, which is worse than that obtained with the RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb67a42-e52c-4569-ad72-5d7414ef4493",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4b3eb-6583-4f1a-b89d-0b4c02122699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SVR model\n",
    "svr = SVR()\n",
    "\n",
    "# Multi output wrapper\n",
    "multi_svr = MultiOutputRegressor(svr)\n",
    "\n",
    "#start\n",
    "start_time = time.time()\n",
    "\n",
    "multi_svr.fit(X_train_scaled , y_train_scaled)\n",
    "\n",
    "#end\n",
    "end_time = time.time()\n",
    "print(f'Execution Time: {end_time - start_time} seconds')\n",
    "\n",
    "y_pred = wrapper.predict(X_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354258d-59ef-4b14-82f4-23b95d0db29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa le librerie necessarie\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assume che 'X' sia il tuo array di features (pmax, negpmax, area, rms, tmax per ogni sensore)\n",
    "# e 'y' sia il tuo array bidimensionale di coordinate (x, y)\n",
    "\n",
    "# Divide il dataset in training e test set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size= 0.2, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Crea una pipeline con uno scaler e un regressore SVR\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Normalizza le feature\n",
    "    ('svr', MultiOutputRegressor(SVR()))  # Utilizza il Support Vector Regressor\n",
    "])\n",
    "\n",
    "# Addestra il modello sulla pipeline con i dati di training\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a683e7-ddff-4848-81a3-d158f98ae694",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_valid)\n",
    "\n",
    "# Calcola l'errore quadratico medio (MSE) per valutare le prestazioni\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7593915-1231-4012-a02e-732e09512501",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43bb9f-9b4e-4674-bb14-66ee44e5f260",
   "metadata": {},
   "source": [
    "# Local evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96ff55-d849-4d12-8906-0ad47176c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model through Euclidean distance\n",
    "def distance_evaluation(y_true, y_pred):\n",
    "    # distance\n",
    "    distances = np.sqrt(np.sum((y_true - y_pred)**2, axis=1))\n",
    "    # distance mean\n",
    "    result = np.mean(distances)\n",
    "    return result\n",
    "\n",
    "result = distance_evaluation(y_valid, y_pred)\n",
    "print('Distance Evaluation Result:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6e53f-0526-4161-ad0c-21768e59ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance Evaluation Result: 4.297281186491101\n",
    " #con 1000 estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39e7c7-2fac-446b-9914-5518e77ea462",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c22dd-c166-412c-9507-dbee9da8c8bb",
   "metadata": {},
   "source": [
    "# Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d154b0-55b9-4f4d-bae4-e6d01c5bbdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluation dataaset\n",
    "df_eval = pd.read_csv('evaluation.csv',index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a411b50-75fd-4573-803b-639eef80d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the same transformations to the evaluation dataset as applied to the development dataset\n",
    "\n",
    "# remove noise features\n",
    "for i in noise_features:\n",
    "   df_eval = df_eval.drop(columns=[f'pmax[{i}]',f'negpmax[{i}]',f'area[{i}]',f'tmax[{i}]',f'rms[{i}]'])\n",
    "\n",
    "# remove the less important features\n",
    "for i in no_noise_features:\n",
    "    df_eval = df_eval.drop(columns=[f'tmax[{i}]',f'rms[{i}]',f'area[{i}]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65212b81-e666-4bf0-a26d-bef08c68c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the predictions\n",
    "X_eval = df_eval.values\n",
    "\n",
    "y_pred = reg.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ec809-8e35-4fec-b83d-cc0d33ce032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the correct format for evaluation\n",
    "df_pred = pd.DataFrame(y_pred, columns=['Predicted1','Predicted2'])\n",
    "\n",
    "df_pred['Id'] = df_pred.index\n",
    "\n",
    "df_pred['Predicted'] = df_pred[['Predicted1', 'Predicted2']].astype(str).agg('|'.join, axis=1)\n",
    "\n",
    "df_pred = df_pred.drop(columns=['Predicted1', 'Predicted2'])\n",
    "\n",
    "df_pred.to_csv('pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
